import os
import importlib
from typing import Optional, List, Dict, Tuple
import io
import random
from PIL import Image, ImageDraw, ImageFont

# Lightweight, optional AI provider helpers.
# - Gemini (google-genai): set GOOGLE_API_KEY or GEMINI_API_KEY
# - OpenAI (openai): set OPENAI_API_KEY
# If the library or key is missing, functions return None gracefully.


def _get_gemini_client():
    """Return a google-genai client if available and keyed, else None.
    Prefer GEMINI_API_KEY to avoid conflicts with other Google APIs (e.g., Cloud TTS).
    """
    api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        return None
    try:
        genai = importlib.import_module("google.genai")
        client = genai.Client(api_key=api_key)
        return client
    except Exception:
        return None


def _get_openai_client():
    """Return an OpenAI client if available and keyed, else None."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return None
    try:
        openai = importlib.import_module("openai")
        return openai
    except Exception:
        return None


def summarize_text(text: str, max_tokens: int = 120) -> Optional[str]:
    """Attempt to summarize text using Gemini first, then OpenAI. Returns None if unavailable."""
    if not text or not isinstance(text, str):
        return None

    # 1) Try Gemini
    client = _get_gemini_client()
    if client is not None:
        try:
            # Keep the prompt short; models handle long input, but we may truncate.
            prompt = (
                "Resume en 3-4 l√≠neas, claro y conciso, en espa√±ol, destacando ideas clave y tono accesible.\n\n"
            )
            # You can also use a structured config if needed; the simple call works for text-only.
            resp = client.models.generate_content(
                model="gemini-1.5-flash",
                contents=prompt + text[:6000]  # safety limit
            )
            # Extract text safely
            summary = None
            try:
                summary = resp.candidates[0].content.parts[0].text
            except Exception:
                summary = getattr(resp, "text", None)
            if summary:
                return summary.strip()
        except Exception:
            pass

    # 2) Try OpenAI as fallback
    openai = _get_openai_client()
    if openai is not None:
        try:
            # Use Responses API if available, else chat.completions as fallback
            # Prefer a small fast model name; user can configure via env later if desired
            if hasattr(openai, "chat"):
                r = openai.chat.completions.create(
                    model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                    messages=[
                        {
                            "role": "system",
                            "content": "Eres un asistente que resume textos en espa√±ol de forma breve (3-4 l√≠neas) y clara."
                        },
                        {"role": "user", "content": text[:6000]},
                    ],
                    max_tokens=max_tokens,
                    temperature=0.4,
                )
                return (r.choices[0].message.content or "").strip()
        except Exception:
            pass

    # 3) Fallback: simple heuristic summary (no external AI required)
    try:
        import re
        # Normalize whitespace
        raw = re.sub(r"\s+", " ", text).strip()
        # Split into sentences
        sents = re.split(r"(?<=[.!?¬°¬ø])\s+", raw)
        sents = [s.strip() for s in sents if s and len(s.strip()) > 2]
        if not sents:
            return raw[:300]
        # If very short, just return it
        if len(raw) <= 300:
            return raw
        # Pick 3 representative sentences: first, middle, last
        picks = []
        picks.append(sents[0])
        if len(sents) >= 3:
            picks.append(sents[len(sents)//2])
            picks.append(sents[-1])
        elif len(sents) == 2:
            picks.append(sents[1])
        # Join and trim to a compact length
        out = ' '.join(picks).strip()
        if len(out) > 600:
            out = out[:600].rsplit(' ', 1)[0] + '‚Ä¶'
        return out
    except Exception:
        # Last resort: truncate beginning of text
        try:
            return (text[:600] + ('‚Ä¶' if len(text) > 600 else ''))
        except Exception:
            return None


def analyze_content(text: str) -> Optional[dict]:
    """
    Analiza contenido y devuelve un dict con campos:
    - summary (string)
    - topics (list[str])
    - tone (string)
    - complexity (string)
    - warnings (list[str])
    - faqs (list[str])
    Usa Gemini u OpenAI si hay claves; si no, heur√≠stica b√°sica.
    """
    if not text or not isinstance(text, str):
        return None

    # Intentar con Gemini
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = (
                "Eres un analista. Devuelve JSON con: summary (3-4 l√≠neas), topics (5 palabras clave), "
                "tone (amigable/neutral/t√©cnico/literario), complexity (baja/media/alta), warnings (si hay temas sensibles), "
                "faqs (3-5 preguntas frecuentes). Responde SOLO JSON v√°lido.\n\nTexto:\n"
            )
            resp = client.models.generate_content(
                model="gemini-1.5-flash",
                contents=prompt + text[:8000]
            )
            # Intentar parsear JSON desde el texto de salida
            parsed = None
            try:
                raw = resp.candidates[0].content.parts[0].text
            except Exception:
                raw = getattr(resp, "text", "")
            import json
            parsed = json.loads(raw)
            return parsed
        except Exception:
            pass

    # Intentar con OpenAI
    openai = _get_openai_client()
    if openai is not None:
        try:
            import json as _json
            sysmsg = (
                "Devuelve SOLO JSON con: summary, topics (array), tone, complexity, warnings (array), faqs (array)."
            )
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role": "system", "content": sysmsg},
                    {"role": "user", "content": text[:8000]},
                ],
                temperature=0.3,
            )
            content = r.choices[0].message.content or "{}"
            return _json.loads(content)
        except Exception:
            pass

    # Heur√≠stica b√°sica si no hay IA
    lowered = text.lower()
    keywords = []
    for kw in ["historia", "ciencia", "tecnolog√≠a", "arte", "biolog√≠a", "novela", "cuento", "noticia", "aprendizaje"]:
        if kw in lowered:
            keywords.append(kw)
    tone = "t√©cnico" if any(k in lowered for k in ["investigaci√≥n", "m√©todo", "datos", "teor√≠a"]) else "neutral"
    complexity = "alta" if len(text) > 12000 else ("media" if len(text) > 4000 else "baja")
    warnings = []
    for w in ["violencia", "sexo", "terror", "muerte", "discriminaci√≥n"]:
        if w in lowered:
            warnings.append(w)
    summary = (text[:400] + "...") if len(text) > 450 else text
    faqs = [
        "¬øCu√°l es la idea principal del contenido?",
        "¬øQu√© conceptos clave se presentan?",
        "¬øPara qui√©n est√° dirigido este contenido?",
    ]
    return {
        "summary": summary,
        "topics": keywords[:5],
        "tone": tone,
        "complexity": complexity,
        "warnings": warnings,
        "faqs": faqs,
    }


def expand_query(query: str) -> List[str]:
    """Expand a user query into related keywords using Gemini/OpenAI if available; else tokenized words."""
    if not query:
        return []
    # Try Gemini
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = (
                "Devuelve una lista separada por comas de 8-12 palabras/phrases relacionadas sem√°nticamente con: "
                f"'{query}'. SOLO la lista, sin explicaciones."
            )
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            txt = None
            try:
                txt = resp.candidates[0].content.parts[0].text
            except Exception:
                txt = getattr(resp, "text", "")
            if txt:
                items = [w.strip() for w in txt.split(',') if w.strip()]
                base = [t.strip() for t in query.lower().split() if t.strip()]
                return list(dict.fromkeys(base + items))
        except Exception:
            pass
    # Try OpenAI
    openai = _get_openai_client()
    if openai is not None:
        try:
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[{"role":"system","content":"Devuelve solo una lista separada por comas de keywords relacionadas."},
                         {"role":"user","content":query}],
                temperature=0.3,
            )
            txt = r.choices[0].message.content or ''
            items = [w.strip() for w in txt.split(',') if w.strip()]
            base = [t.strip() for t in query.lower().split() if t.strip()]
            return list(dict.fromkeys(base + items))
        except Exception:
            pass
    # Fallback: tokens del query
    return [t.strip() for t in query.lower().split() if t.strip()]


def translate_text(text: str, target_lang: str = 'en') -> Optional[str]:
    """Translate text via Gemini/OpenAI; returns None if unavailable."""
    if not text:
        return None
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = f"Traduce al {target_lang} conservando el sentido y tono:\n\n{text[:8000]}"
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            try:
                out = resp.candidates[0].content.parts[0].text
            except Exception:
                out = getattr(resp, "text", None)
            return out.strip() if out else None
        except Exception:
            pass
    openai = _get_openai_client()
    if openai is not None:
        try:
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[{"role":"system","content":f"Traduce al {target_lang}."},
                         {"role":"user","content":text[:8000]}],
                temperature=0.3,
            )
            return (r.choices[0].message.content or '').strip()
        except Exception:
            pass
    return None


def detect_language(text: str) -> str:
    """Very naive language detection; prefer AI if available."""
    if not text:
        return 'und'
    client = _get_gemini_client()
    if client is not None:
        try:
            resp = client.models.generate_content(
                model="gemini-1.5-flash",
                contents=f"Detecta el idioma (c√≥digo ISO como es,en,pt,fr,de) y responde SOLO el c√≥digo:\n\n{text[:2000]}"
            )
            code = None
            try:
                code = resp.candidates[0].content.parts[0].text
            except Exception:
                code = getattr(resp, "text", "und")
            return code.strip().lower()[:5]
        except Exception:
            pass
    # naive
    s = text.lower()
    if any(w in s for w in [' el ', ' la ', ' de ', ' que ']):
        return 'es'
    if any(w in s for w in [' the ', ' of ', ' and ']):
        return 'en'
    return 'und'


def generate_quiz(text: str, n: int = 5) -> Optional[List[Dict[str, str]]]:
    """Generate simple Q&A pairs from text via Gemini/OpenAI; returns list of {q,a}."""
    if not text:
        return None
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = (
                f"Genera {n} preguntas y respuestas cortas en espa√±ol sobre el texto. Formato JSON array de objetos con 'q' y 'a'.\n\n{text[:8000]}"
            )
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            raw = None
            try:
                raw = resp.candidates[0].content.parts[0].text
            except Exception:
                raw = getattr(resp, "text", "[]")
            import json
            return json.loads(raw)
        except Exception:
            pass
    openai = _get_openai_client()
    if openai is not None:
        try:
            import json as _json
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[{"role":"system","content":"Devuelve SOLO JSON array con objetos {q,a}."},
                         {"role":"user","content":text[:8000]}],
                temperature=0.4,
            )
            return _json.loads(r.choices[0].message.content or '[]')
        except Exception:
            pass
    # Fallback heur√≠stico: preguntas gen√©ricas
    return [{"q":"¬øCu√°l es la idea principal?","a":"Trata sobre los conceptos clave presentados en el texto."}]


def generate_notes(text: str) -> Optional[List[str]]:
    """
    Genera notas de estudio orientadas al aprendizaje:
    - 3-5 puntos clave (prefijo "Punto:")
    - 2-3 preguntas educativas (prefijo "Pregunta:")
    Usa Gemini/OpenAI si est√°n disponibles; de lo contrario, heur√≠stica local.
    Devuelve lista de strings (vi√±etas).
    """
    if not text:
        return None

    # 1) Gemini (preferido)
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = (
                "Eres un tutor. Devuelve una lista en vi√±etas (una por l√≠nea) con 5-7 elementos en espa√±ol.\n"
                "Incluye: 3-5 puntos clave y 2-3 preguntas educativas de comprensi√≥n o aplicaci√≥n.\n"
                "Formato exacto por l√≠nea:\n"
                "- Punto: <idea breve y concreta>\n"
                "- Pregunta: <pregunta enfocada y √∫til>\n\n"
                "S√© espec√≠fico, claro y evita redundancias. Texto:\n\n" + text[:8000]
            )
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            out = None
            try:
                out = resp.candidates[0].content.parts[0].text
            except Exception:
                out = getattr(resp, "text", None)
            if out:
                lines = [l.strip() for l in out.split('\n') if l.strip()]
                # normalizar vi√±etas
                bullets = []
                for l in lines:
                    l = l.lstrip('-‚Ä¢ ').strip()
                    bullets.append(l)
                # recortar a 7
                return bullets[:7] if bullets else None
        except Exception:
            pass

    # 2) OpenAI como alternativa
    openai = _get_openai_client()
    if openai is not None:
        try:
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role":"system","content":(
                        "Eres un tutor. Devuelve 5-7 l√≠neas. Cada l√≠nea debe ser una vi√±eta con el formato: "
                        "'Punto: ...' (ideas) o 'Pregunta: ...' (comprensi√≥n). Espa√±ol, concreto, √∫til."
                    )},
                    {"role":"user","content":text[:8000]},
                ],
                temperature=0.4,
            )
            txt = (r.choices[0].message.content or '')
            lines = [l.strip() for l in txt.split('\n') if l.strip()]
            bullets = [l.lstrip('-‚Ä¢ ').strip() for l in lines]
            return bullets[:7] if bullets else None
        except Exception:
            pass

    # 3) Fallback local (sin IA)
    try:
        import re
        # Normalizar y segmentar oraciones
        clean = re.sub(r"\s+", " ", text).strip()
        sents = re.split(r"(?<=[.!?¬°¬ø])\s+", clean)
        sents = [s.strip() for s in sents if s and len(s.strip()) > 30]
        # Palabras clave de instrucci√≥n para priorizar oraciones √∫tiles
        keywords = [
            "definici√≥n", "objetivo", "importancia", "clave", "resultado", "evidencia",
            "causa", "efecto", "consecuencia", "ejemplo", "proceso", "paso", "beneficio",
        ]
        def score(sent: str) -> int:
            sc = 0
            low = sent.lower()
            for k in keywords:
                if k in low:
                    sc += 2
            # favorecer oraciones de longitud media
            ln = len(sent)
            if 60 <= ln <= 220:
                sc += 2
            return sc
        ranked = sorted(sents, key=score, reverse=True)
        points = [f"Punto: {s}" for s in ranked[:4]]
        # Preguntas heur√≠sticas
        questions = [
            "Pregunta: ¬øCu√°l es la idea principal del texto?",
            "Pregunta: ¬øQu√© causas, efectos o evidencias se mencionan?",
            "Pregunta: ¬øC√≥mo podr√≠as aplicar este contenido en una situaci√≥n real?",
        ]
        bullets = (points + questions)[:7]
        return bullets or [clean[:120] + ('‚Ä¶' if len(clean) > 120 else '')]
    except Exception:
        # √öltimo recurso
        return [text[:200] + ('‚Ä¶' if len(text) > 200 else '')]


def _heuristic_complexity_metrics(text: str) -> Tuple[float, float]:
    """Return (avg_sentence_len, unique_ratio) as quick proxies for complexity."""
    import re
    sents = re.split(r'[.!?]\s+', text.strip())
    sents = [s for s in sents if s]
    if not sents:
        return 0.0, 0.0
    words = [w for w in re.findall(r"\b\w+\b", text.lower())]
    avg_len = sum(len(re.findall(r"\b\w+\b", s)) for s in sents) / max(1, len(sents))
    unique_ratio = len(set(words)) / max(1, len(words))
    return avg_len, unique_ratio


def analyze_accessibility(text: str) -> Optional[Dict]:
    """Analyze text for accessibility: complexity level, recommended speed, pause points, chapter summaries."""
    if not text:
        return None
    # Try AI first for rich output
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = (
                "Analiza el texto para accesibilidad. Devuelve JSON con: "
                "complexity_level (baja|media|alta), recommended_speed (0.8-1.3), "
                "pauses (array de frases o indicaciones breves), chapters (array de objetos {title, summary}). "
                "S√© conciso, √∫til y claro. Texto:\n\n" + text[:8000]
            )
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            raw = None
            try:
                raw = resp.candidates[0].content.parts[0].text
            except Exception:
                raw = getattr(resp, "text", "{}")
            import json
            data = json.loads(raw)
            # basic guards
            if isinstance(data, dict):
                return data
        except Exception:
            pass
    openai = _get_openai_client()
    if openai is not None:
        try:
            import json as _json
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[{"role":"system","content":"Devuelve SOLO JSON con keys: complexity_level, recommended_speed, pauses, chapters[{title,summary}]"},
                         {"role":"user","content":text[:8000]}],
                temperature=0.2,
            )
            return _json.loads(r.choices[0].message.content or '{}')
        except Exception:
            pass
    # Heuristic fallback
    avg_len, uniq = _heuristic_complexity_metrics(text)
    if avg_len >= 25 or uniq >= 0.55: level = 'alta'
    elif avg_len >= 15 or uniq >= 0.45: level = 'media'
    else: level = 'baja'
    speed = 0.9 if level=='alta' else (1.0 if level=='media' else 1.15)
    # pauses: after long sentences or double newlines
    import re
    raw_sents = re.split(r'([.!?])', text)
    pauses = []
    # recombine sentence with punctuation tokens
    cur = ''
    for part in raw_sents:
        cur += part
        if part in '.!?':
            if len(cur.split()) >= 18:
                pauses.append(cur.strip())
            cur = ''
    # chapters: naive split by headings
    chapters = []
    lines = text.splitlines()
    buffer = []
    current_title = None
    def flush():
        nonlocal buffer, current_title, chapters
        if buffer:
            bl = ' '.join(buffer).strip()
            if bl:
                summ = summarize_text(bl) or bl.split('.')[0][:220]
                chapters.append({'title': current_title or 'Secci√≥n', 'summary': summ})
            buffer = []
    for ln in lines:
        if re.match(r'^(cap[i√≠]tulo|secci[o√≥]n|\d+\.|#)', ln.strip(), re.IGNORECASE):
            flush()
            current_title = ln.strip()
        else:
            buffer.append(ln)
    flush()
    return {
        'complexity_level': level,
        'recommended_speed': round(speed, 2),
        'pauses': pauses[:12],
        'chapters': chapters[:10],
    }


def support_answer(message: str, context: Optional[str] = None) -> str:
    """Answer support questions about using the app. AI if available; fallback FAQ-like text."""
    if not message:
        return "¬øEn qu√© puedo ayudarte? Puedes preguntar por conversi√≥n a audio, biblioteca, reproductor o recomendaciones."
    client = _get_gemini_client()
    if client is not None:
        try:
            sys = "Eres el asistente de soporte de SinSay. S√© claro, breve y pr√°ctico."
            prompt = (context or sys) + "\n\nUsuario: " + message
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            try:
                return resp.candidates[0].content.parts[0].text.strip()
            except Exception:
                return (getattr(resp, 'text', None) or '').strip()
        except Exception:
            pass
    openai = _get_openai_client()
    if openai is not None:
        try:
            r = openai.chat.completions.create(
                model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[{"role":"system","content":"Eres el soporte de SinSay. S√© claro y breve."},
                         {"role":"user","content":message}],
                temperature=0.2,
            )
            return (r.choices[0].message.content or '').strip()
        except Exception:
            pass
    # fallback
    m = message.lower()
    if 'convert' in m or 'audio' in m:
        return "Para convertir un archivo a audio, ve a Reproductor > Generar audio, selecciona el archivo y elige una voz."
    if 'biblioteca' in m or 'library' in m:
        return "En Biblioteca puedes filtrar por categor√≠a, nivel y buscar por t√≠tulo. Haz click en una tarjeta para abrirla."
    if 'reproductor' in m or 'play' in m:
        return "En el Reproductor puedes pausar, adelantar 15s, ajustar volumen y ver texto sincronizado si est√° disponible."
    return "Puedo ayudarte con: convertir a audio, navegar la biblioteca, reproducir contenidos y recomendaciones personalizadas."


def moderate_text(text: str) -> Dict:
    """Moderate content: flags for inappropriate/sensitive/spam/quality issues; AI-backed with heuristic fallback."""
    flags = { 'inappropriate': False, 'sensitive_content': False, 'spam': False, 'quality_issues': [] }
    if not text:
        return flags
    client = _get_gemini_client()
    if client is not None:
        try:
            prompt = (
                "Analiza el texto y devuelve JSON con flags: inappropriate(bool), sensitive_content(bool), spam(bool), "
                "quality_issues(array de strings breves). S√© estricto con lenguaje expl√≠cito/violencia extrema y datos personales. Texto:\n\n" + text[:8000]
            )
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
            raw = None
            try:
                raw = resp.candidates[0].content.parts[0].text
            except Exception:
                raw = getattr(resp, 'text', '{}')
            import json
            data = json.loads(raw)
            if isinstance(data, dict):
                return {**flags, **data}
        except Exception:
            pass
    # heuristic fallback
    low = text.lower()
    bad_terms = ['odio', 'matar', 'violaci√≥n', 'sexo expl√≠cito']
    if any(t in low for t in bad_terms): flags['inappropriate'] = True
    if any(w in low for w in ['suicidio','autolesi√≥n','terrorismo','droga']): flags['sensitive_content'] = True
    if len(low) > 2000 and low.count('http') > 10: flags['spam'] = True
    # quality
    import re
    if not re.search(r'[\.\!\?]', text): flags['quality_issues'].append('Falta puntuaci√≥n en el texto')
    avg_len, _ = _heuristic_complexity_metrics(text)
    if avg_len > 40: flags['quality_issues'].append('Oraciones excesivamente largas')
    return flags


# --- Simple AI-ish cover generator (local, no external calls) ---
_CATEGORY_STYLES = {
    'novela': {'bg': (37, 99, 235), 'fg': (255,255,255), 'emoji': 'üìñ'},
    'cuento': {'bg': (16, 185, 129), 'fg': (0,0,0), 'emoji': '‚ú®'},
    'ciencia': {'bg': (99, 102, 241), 'fg': (255,255,255), 'emoji': 'üî¨'},
    'tecnologia': {'bg': (55, 65, 81), 'fg': (255,255,255), 'emoji': 'üß†'},
    'tecnolog√≠a': {'bg': (55, 65, 81), 'fg': (255,255,255), 'emoji': 'üß†'},
    'historia': {'bg': (180, 83, 9), 'fg': (255,255,255), 'emoji': 'üè∫'},
    'arte': {'bg': (236, 72, 153), 'fg': (255,255,255), 'emoji': 'üé®'},
    'biologia': {'bg': (34, 197, 94), 'fg': (0,0,0), 'emoji': 'üß¨'},
    'biolog√≠a': {'bg': (34, 197, 94), 'fg': (0,0,0), 'emoji': 'üß¨'},
    'educacion': {'bg': (2, 132, 199), 'fg': (255,255,255), 'emoji': 'üéì'},
    'educaci√≥n': {'bg': (2, 132, 199), 'fg': (255,255,255), 'emoji': 'üéì'},
    'poesia': {'bg': (168, 85, 247), 'fg': (255,255,255), 'emoji': 'üïäÔ∏è'},
    'poes√≠a': {'bg': (168, 85, 247), 'fg': (255,255,255), 'emoji': 'üïäÔ∏è'},
}


def _pick_style(category: Optional[str]) -> Dict[str, object]:
    if not category:
        return {'bg': (31, 41, 55), 'fg': (255,255,255), 'emoji': 'üìö'}
    key = category.strip().lower()
    for k, v in _CATEGORY_STYLES.items():
        if k in key:
            return v
    # fallback random pleasant color
    palettes = [
        (59, 130, 246), (99, 102, 241), (236, 72, 153), (34, 197, 94), (234, 179, 8), (2, 132, 199)
    ]
    return {'bg': random.choice(palettes), 'fg': (255,255,255), 'emoji': 'üìò'}


def _load_font(size: int) -> ImageFont.FreeTypeFont:
    # Try common fonts; fallback to default bitmap font
    for name in [
        "arial.ttf", "Segoe UI.ttf", "DejaVuSans.ttf", "NotoSans-Regular.ttf",
    ]:
        try:
            return ImageFont.truetype(name, size)
        except Exception:
            continue
    try:
        return ImageFont.load_default()
    except Exception:
        return ImageFont.load_default()


def generate_cover_image_bytes(category: Optional[str], title: Optional[str], subtitle: Optional[str] = None,
                               size: Tuple[int, int] = (768, 1024)) -> Optional[bytes]:
    """Generate a simple cover as JPEG bytes based on category and title.
    No external AI calls; safe offline fallback.
    """
    try:
        w, h = size
        style = _pick_style(category)
        img = Image.new('RGB', (w, h), color=style['bg'])
        draw = ImageDraw.Draw(img)

        # Top emoji/icon
        emoji = style.get('emoji', 'üìö')
        emoji_font = _load_font(int(h * 0.18))
        ef_w, ef_h = draw.textsize(emoji, font=emoji_font)
        draw.text(((w - ef_w) / 2, int(h * 0.08)), emoji, font=emoji_font, fill=style['fg'])

        # Title (wrapped)
        title_text = (title or 'Sin t√≠tulo').strip()
        max_title_width = int(w * 0.80)
        # progressively reduce font size to fit lines
        tsize = int(h * 0.065)
        while tsize >= 26:
            font = _load_font(tsize)
            # naive wrap by measuring words
            words = title_text.split()
            lines = []
            cur = ''
            for wd in words:
                test = (cur + ' ' + wd).strip()
                tw, th = draw.textsize(test, font=font)
                if tw <= max_title_width:
                    cur = test
                else:
                    if cur:
                        lines.append(cur)
                    cur = wd
            if cur:
                lines.append(cur)
            if len(lines) <= 3:
                break
            tsize -= 4
        # Center title block around 45% height
        total_h = sum(draw.textsize(l, font=font)[1] for l in lines) + (len(lines)-1)*8
        y = int(h * 0.40 - total_h / 2)
        for l in lines:
            tw, th = draw.textsize(l, font=font)
            draw.text(((w - tw)/2, y), l, font=font, fill=style['fg'])
            y += th + 8

        # Subtitle/category chip at bottom
        chip = (subtitle or category or '').strip()
        if chip:
            chip_font = _load_font(int(h * 0.035))
            pad_x, pad_y = 16, 10
            ctw, cth = draw.textsize(chip, font=chip_font)
            box_w, box_h = ctw + pad_x*2, cth + pad_y*2
            bx, by = int((w - box_w)/2), int(h*0.80)
            # semi-transparent-like effect via darker bg
            bg = tuple(max(0, int(style['bg'][i] * 0.7)) for i in range(3))
            draw.rounded_rectangle([bx, by, bx+box_w, by+box_h], radius=12, fill=bg)
            draw.text((bx+pad_x, by+pad_y), chip, font=chip_font, fill=style['fg'])

        buf = io.BytesIO()
        img.save(buf, format='JPEG', quality=88)
        return buf.getvalue()
    except Exception:
        return None
